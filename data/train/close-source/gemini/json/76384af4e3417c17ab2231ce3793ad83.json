{
  "query": "The essence of Multiple Imputation, especially explained from a Bayesian perspective; the advantages and limitations of complete case analysis, particularly whether CCA is unbiased when used to estimate regression coefficients for categorical outcome variables.",
  "chain_of_research": {
    "plan_1": {
      "title": "Research Approach\nInitial Focus Areas\nNext Steps",
      "description": "I'm starting a comprehensive investigation into Multiple Imputation (MI), particularly from a Bayesian perspective, and Complete Case Analysis (CCA). My goal is to understand their core principles, methodologies, advantages, and limitations, especially concerning their application to regression coefficients with categorical outcome variables. I'll also be comparing these two methods and exploring specific scenarios where CCA might yield unbiased results.\nMy initial research will concentrate on the fundamental aspects of Multiple Imputation, including its general steps and how Bayesian inference is applied within this framework, looking into concepts like prior and posterior predictive distributions. Simultaneously, I'll be examining Complete Case Analysis, focusing on its definition, perceived advantages, and significant drawbacks, with a keen eye on understanding when it leads to biased regression coefficients, particularly when dealing with categorical outcome variables.\nI'm planning to use a combination of search and browsing to gather information from academic papers, statistical textbooks, and university resources. I'll prioritize highly reputable sources to ensure the accuracy and depth of the information I collect. My next actions will involve executing these initial searches to build a foundational understanding of both MI and CCA."
    },
    "search_1": [
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC8057424",
      "https://www.bookdown.org/rwnahhas/RMPH/datasumm-missing.html",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC4233994",
      "https://www.numberanalytics.com/blog/ultimate-guide-multiple-imputation-bayesian-statistics",
      "https://www.numberanalytics.com/blog/essential-guide-multiple-imputation",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC4638176",
      "https://bise.wceruw.org/documents/Kaplan_Yavuz.2019.pdf",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC5496677",
      "https://stefvanbuuren.name/fimd/sec-simplesolutions.html",
      "https://stats.oarc.ucla.edu/stata/seminars/mi_in_stata_pt1_new",
      "https://www.researchgate.net/publication/361158443_Uncertainty_limits_the_use_of_power_analysis",
      "https://agabrioblog.onrender.com/missmethods/complete-case-analysis",
      "https://fastercapital.com/term/statistical-analysis-limitations.html",
      "https://stefvanbuuren.name/fimd/sec-when.html"
    ],
    "observation_1": "I've started to build a solid understanding of Multiple Imputation (MI). I've learned that MI is a statistical technique designed to handle missing data by creating multiple complete datasets, each with different plausible values for the missing entries. This approach is crucial because it accounts for the uncertainty associated with missing data, leading to less biased parameter estimates compared to single imputation methods. The process generally involves three key steps: imputing missing values to create multiple datasets, analyzing each dataset separately, and then pooling the results using Rubin's Rules to obtain a single, robust estimate.",
    "plan_2": {
      "title": "Bayesian Perspective on MI\nAlgorithms for Bayesian MI\nInsights into Complete Case Analysis\nNext Research Steps",
      "description": "Delving deeper into the Bayesian perspective of MI, I've found that it leverages Bayes' theorem to update the probability of a hypothesis based on new data, providing a natural framework for modeling uncertainty in missing data. Bayesian imputation methods generate imputed values by drawing from the posterior distribution of the missing data, often incorporating prior distributions to enhance accuracy. While powerful, fully Bayesian imputation models can be computationally intensive due to their complexity.\nI've identified two primary computational approaches for Bayesian MI: Markov Chain Monte Carlo (MCMC) methods and Fully Conditional Specification (FCS), also known as Chained Equations (MICE). MCMC methods generate imputations by drawing from the posterior distribution, directly modeling the uncertainty but being computationally demanding. FCS, on the other hand, specifies an imputation model for each variable with missing values based on other variables, offering flexibility but requiring careful model specification. I've also noted the mention of the 'mice' R package as a common implementation of the chained equations approach.\nMy research into Complete Case Analysis (CCA), also known as listwise deletion, confirms its simplicity and ease of implementation as its main advantages. However, a significant insight is that CCA can lead to biased estimates and a loss of statistical power, especially when the data are not Missing Completely at Random (MCAR). If the missingness is Missing at Random (MAR) or Missing Not at Random (MNAR), CCA is generally not recommended. I've also found specific, limited scenarios where CCA might be unbiased for regression coefficients, such as when missing data occur only in the outcome variable (Y) or in certain logistic regression contexts where missingness depends only on Y, but not on both Y and predictors (X).\nI have gathered a good initial understanding of both Multiple Imputation and Complete Case Analysis. My next step is to browse the identified relevant sources in parallel to extract more detailed information. I will focus on elaborating on the theoretical framework of Bayesian MI, understanding the computational nuances of MCMC and FCS, detailing the limitations of CCA regarding statistical power and representativeness, and thoroughly investigating the conditions under which CCA might yield unbiased regression coefficients, particularly for categorical outcome variables. I will also work on building a comprehensive comparison between MI and CCA."
    },
    "search_2": [
      "https://agabrioblog.onrender.com/missmethods/complete-case-analysis",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC8057424",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC4233994",
      "https://stefvanbuuren.name/fimd/sec-when.html",
      "https://www.numberanalytics.com/blog/essential-guide-multiple-imputation",
      "https://bise.wceruw.org/documents/Kaplan_Yavuz.2019.pdf",
      "https://fastercapital.com/term/statistical-analysis-limitations.html",
      "https://stats.oarc.ucla.edu/stata/seminars/mi_in_stata_pt1_new",
      "https://www.bookdown.org/rwnahhas/RMPH/datasumm-missing.html",
      "https://www.numberanalytics.com/blog/ultimate-guide-multiple-imputation-bayesian-statistics",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC5496677"
    ],
    "observation_2": "I've gained a clearer understanding of the fundamental principles of Multiple Imputation (MI), including its three-step process: imputation, analysis, and pooling using Rubin's Rules. This approach is crucial for reflecting the uncertainty associated with missing data and reducing bias. From a Bayesian perspective, MI involves drawing from the posterior predictive distribution of missing data, often incorporating prior knowledge. I've also identified key algorithms like Markov Chain Monte Carlo (MCMC) and Fully Conditional Specification (FCS) or Chained Equations (MICE), noting their strengths and computational considerations.",
    "plan_3": {
      "title": "Deepening Insights into Complete Case Analysis\nAddressing Information Gaps\nNext Steps in Research",
      "description": "My research confirms that Complete Case Analysis (CCA) is simple to implement, but its main drawbacks are potential bias and loss of efficiency, especially when data are not Missing Completely at Random (MCAR). I've learned that if the missingness is Missing at Random (MAR) or Missing Not at Random (MNAR), CCA can lead to biased parameter estimates. I've also identified specific, limited scenarios where CCA might yield unbiased regression coefficients, such as when missing data are confined to the outcome variable or in certain logistic regression cases where missingness depends only on the outcome.\nWhile I have a good foundation, I've identified some key information gaps. I need to delve deeper into how CCA specifically leads to a loss of statistical power and affects representativeness. Crucially, I need to find more targeted information on whether CCA is unbiased when used to estimate regression coefficients for categorical outcome variables, as the current sources only provide limited scenarios or general statements about bias. Finally, I aim to develop a more comprehensive comparison between MI and CCA, detailing their assumptions, strengths, and weaknesses.\nMy next steps will focus on targeted searches to fill these identified gaps. I will specifically look for academic literature and statistical resources that discuss the loss of statistical power and representativeness in Complete Case Analysis, and critically, its bias when dealing with categorical outcome variables in regression models. I will also seek out comprehensive comparisons between Multiple Imputation and Complete Case Analysis to fully articulate their respective advantages and disadvantages."
    },
    "search_3": [
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC4957845/#:~:text=Complete%20case%20(CC)%20analysis%20is,as%20the%20preferred%20analytical%20approach.",
      "https://www.researchgate.net/publication/305625461_Is_using_multiple_imputation_better_than_complete_case_analysis_for_estimating_a_prevalence_risk_difference_in_randomized_controlled_trials_when_binary_outcome_observations_are_missing",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC3217865",
      "https://thedecisionlab.com/biases/representativeness-heuristic",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC7705610",
      "https://www.researchgate.net/publication/364535983_The_introduction_and_analysis_of_Representativeness_bias",
      "https://stefvanbuuren.name/fimd/sec-when.html",
      "https://medium.com/@noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32",
      "https://pubmed.ncbi.nlm.nih.gov/31957905",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC6693809",
      "https://stats.oarc.ucla.edu/sas/seminars/multiple-imputation-in-sas/mi_new_1",
      "https://thestatsgeek.com/2015/01/31/when-is-complete-caserecords-logistic-regression-unbiased",
      "https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data/#:~:text=Listwise%20Deletion%20works%20best%20under,bias%20and%20distort%20the%20analysis.",
      "https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data"
    ],
    "observation_3": "I've gained a deeper understanding of the limitations of Complete Case Analysis (CCA). It's clear that while simple, CCA can lead to significant bias and a loss of efficiency, particularly when the missing data mechanism is not Missing Completely at Random (MCAR). This bias inherently implies that the sample used for analysis may not be representative of the original dataset, as removing cases with missing values can alter the underlying data distribution. This directly impacts the generalizability of the findings.",
    "plan_4": {
      "title": "CCA Bias with Categorical Outcomes\nComparing Multiple Imputation and Complete Case Analysis\nConcluding Research and Reporting",
      "description": "A crucial insight I've uncovered relates to the bias of CCA when estimating regression coefficients for categorical outcome variables. While CCA is generally biased if data are not MCAR, I've found that it can be asymptotically unbiased even under certain Missing at Random (MAR) or Missing Not at Random (MNAR) conditions, especially in logistic regression. This occurs if the probability of being a complete case does not depend on the outcome variable after accounting for covariates. However, a significant finding is that Multiple Imputation (MI) can be superior to CCA when missingness involves a categorical covariate, even in scenarios where both methods are technically valid.\nI've solidified my understanding of the comparison between MI and CCA. MI is generally preferred as it preserves sample size and tends to reduce bias, especially when data are MAR. CCA, by contrast, reduces sample size, leading to a loss of statistical power and potentially biased estimates if the MCAR assumption is violated. Interestingly, I've also found some nuanced scenarios where MI might not offer a statistical advantage over CCA for outcomes under MCAR and MAR conditions, and that CCA can be endorsed for per-protocol risk difference analyses in specific contexts. This highlights that the choice between methods isn't always straightforward and depends on the specific missing data mechanism and analytical goals.\nI have now gathered comprehensive information on the fundamental principles of Multiple Imputation, its Bayesian perspective, the advantages and limitations of Complete Case Analysis, and a detailed comparison between the two, including specific insights into CCA's bias with categorical outcome variables. I believe I have sufficient information to address all aspects of the research question. My next step is to synthesize all the gathered information into a final report."
    }
  },
  "final_report": "# **The Essence of Multiple Imputation and Complete Case Analysis: A Bayesian Perspective and Unbiasedness Conditions for Categorical Outcomes**\n\n## **1. Executive Summary**\n\nThis report provides a rigorous examination of two principal methods for handling missing data in statistical analysis: Multiple Imputation (MI) and Complete Case Analysis (CCA). It elucidates the theoretical underpinnings of MI, with a particular emphasis on its Bayesian formulation, and critically assesses the advantages, limitations, and conditions for unbiasedness of CCA, especially when estimating regression coefficients for categorical outcome variables.\n\nMultiple Imputation stands as a robust approach that explicitly accounts for the uncertainty inherent in missing data by generating and combining results from multiple plausible datasets. Bayesian MI further enhances this by integrating prior knowledge, leading to more accurate and stable imputations. Conversely, Complete Case Analysis, while simple to implement, often suffers from significant data loss and potential for biased estimates unless data are Missing Completely at Random (MCAR). However, this report details specific, often overlooked, scenarios where CCA can yield unbiased regression coefficients, even under Missing at Random (MAR) or Missing Not at Random (MNAR) mechanisms, particularly relevant for logistic regression with categorical outcomes. The nuanced trade-offs between these methods, encompassing statistical efficiency, bias, and computational complexity, are thoroughly discussed to guide researchers in optimal methodological selection.\n\n## **2. Introduction to Missing Data in Statistical Analysis**\n\nThe pervasive challenge of missing data in empirical research across various domains, including medicine, social sciences, and economics, is well-documented.1 Its presence can severely compromise the validity of statistical inferences, leading to biased estimates, reduced statistical power, and unreliable conclusions.2 The integrity of data is paramount; flawed data, including missing values, inevitably leads to flawed conclusions.2 The pervasive nature of missing data underscores that it is not merely a data cleaning issue but a fundamental challenge that can undermine the entire statistical analysis process. If data quality is compromised, any subsequent statistical analysis, regardless of its sophistication, is built on a shaky foundation. This highlights the critical importance of proactive data management strategies from the initial study design to its execution, rather than solely relying on post-hoc statistical remedies. Even advanced techniques like Multiple Imputation cannot fully recover information that was never collected or if the missingness mechanism is fundamentally intractable.\n\nUnderstanding the underlying reasons for missingness, known as missing data mechanisms (MDM), is crucial for selecting appropriate handling strategies. These mechanisms are typically categorized into three types:\n\n* **Missing Completely at Random (MCAR):** The probability of data being missing is entirely independent of both observed and unobserved data.1 In such cases, the missingness is truly random and does not inherently introduce bias, meaning that complete-case analysis can produce unbiased estimates.4\n* **Missing at Random (MAR):** The probability of missingness depends on observed data but not on the unobserved data itself.1 For example, if older participants are more likely to have missing income data, but this missingness is fully explained by their age (which is observed), the data are MAR. This mechanism is common and often allows for valid imputation techniques.8\n* **Missing Not at Random (MNAR):** The probability of missingness depends on both observed and unobserved data.1 For instance, if individuals with very high incomes are less likely to report their income, and this non-reporting is not explained by other observed variables, the data are MNAR. This is the most challenging mechanism, as it can introduce bias that is difficult to correct without advanced modeling techniques or strong, untestable assumptions.4\n\nTraditionally, simple methods like complete case analysis (listwise deletion) or single imputation techniques (e.g., mean imputation, last observation carried forward) have been employed.7 However, these simpler methods are often suboptimal, frequently yielding biased results and suboptimal performance.7 More sophisticated techniques, such as Multiple Imputation, have emerged as preferred alternatives due to their ability to provide more robust and accurate estimates.12 The validity of any missing data handling method is intrinsically tied to the underlying missing data mechanism. This is not merely a classification; it is a critical decision point. For instance, while Complete Case Analysis is generally valid only under MCAR, Multiple Imputation is specifically designed for MAR data. This highlights that blindly applying a method without a thorough understanding of the missing data mechanism is a fundamental methodological flaw, potentially leading to invalid conclusions.\n\nA profound challenge in missing data analysis is the inherent difficulty in empirically distinguishing between MAR and MNAR solely from the observed data.8 This unidentifiability means that researchers are often forced to rely on strong, untestable assumptions or domain expertise to infer the missingness mechanism. Consequently, any method assuming MAR (like standard MI) or MCAR (like CCA) could be biased without the analyst's knowledge. This necessitates a shift from purely data-driven decisions to incorporating domain knowledge and conducting sensitivity analyses to assess the robustness of findings.\n\n## **3. Multiple Imputation: Fundamental Principles and Methodology**\n\n### **3.1 Definition and Core Concept: Addressing Uncertainty Through Multiple Datasets**\n\nMultiple Imputation (MI) is a statistical technique designed to handle missing data by replacing each missing value not with a single estimate, but with a set of plausible values.1 This crucial step explicitly reflects the inherent uncertainty about the true missing value.19 Rather than filling in gaps with a single estimate, MI creates several (typically between 5 and 20) complete datasets, each with different imputations for the missing values.19 This multiplicity is key to capturing and propagating the variability introduced by the missingness throughout the analysis.1\n\nThe core innovation of MI is its explicit acknowledgment and quantification of \"imputation uncertainty\".19 Unlike single imputation methods that treat imputed values as observed, artificially reducing variance, MI's generation of multiple datasets allows for the propagation of this uncertainty into the final estimates. This represents a fundamental shift in how missing data are conceptualized—moving from a deterministic \"fill-in-the-gap\" approach to a probabilistic \"what if\" framework. This leads to more conservative and realistic standard errors and confidence intervals, preventing a false sense of precision in study conclusions.19\n\n### **3.2 The Three-Step Process of Multiple Imputation**\n\nThe MI procedure generally follows three distinct phases, often referred to as Rubin's Rules 7:\n\n* **Step 1: Imputation Phase (Fill-in Phase):** Multiple copies (m datasets) of the original data are created.3 In each dataset, missing values are replaced by drawing from the predictive distribution of the missing data, given the observed data.19 This process is random and repeated multiple times to avoid pretending the missing values are known with certainty.20 This phase explicitly accounts for the uncertainty associated with the missing values by generating a range of plausible estimates.1\n* **Step 2: Analysis Phase:** The desired statistical analysis (e.g., regression, ANOVA) is performed independently on each of the 'm' imputed datasets.7 Each analysis yields a parameter estimate and an associated measure of uncertainty, such as a standard error.19\n* **Step 3: Pooling Phase:** The results from the 'm' separate analyses are combined into a single, overall estimate and a combined variance using Rubin's Rules.1 This pooling accounts for both within-imputation variability (the standard error from each individual analysis) and between-imputation variability (the differences observed across the 'm' imputed datasets). This provides a comprehensive measure of total uncertainty, leading to more accurate standard errors and confidence intervals.19 Rubin's Rules are not merely an averaging mechanism; they are a sophisticated statistical framework for combining information from multiple, slightly different, complete datasets. The strength lies in how they partition the total variance into within-imputation and between-imputation components. This allows MI to provide valid inferences even when the individual imputed datasets are \"wrong\" in their specific imputations, as long as the imputation model correctly captures the\n  *distribution* of the missing data. This aggregation effectively leverages a \"wisdom of crowds\" principle in a statistical context, yielding more robust estimates and valid statistical inferences compared to single imputation methods, which often underestimate variance by ignoring imputation uncertainty.\n\n### **3.3 General Advantages of Multiple Imputation**\n\nMultiple Imputation offers several significant advantages over simpler missing data handling methods:\n\n* **Accounts for Imputation Uncertainty:** By generating multiple datasets, MI explicitly reflects the uncertainty regarding missing values, leading to more accurate standard errors and confidence intervals.1\n* **Reduces Bias:** MI tends to yield less biased parameter estimates compared to single imputation methods or complete case analysis, especially under MAR conditions.1\n* **Improves Validity of Statistical Inferences:** It provides a more accurate representation of the true variability in the data, thereby enhancing the validity of statistical inferences.1\n* **Maximizes Data Use:** Unlike Complete Case Analysis, MI utilizes all available data, preserving sample size and statistical power, which is critical for robust analysis.14\n\nWhile bias reduction is a primary advantage 1, the true strength of MI lies in its holistic improvement of statistical inference. By correctly accounting for uncertainty, it not only provides more accurate point estimates but also more reliable measures of precision (standard errors, confidence intervals). This means researchers can draw more trustworthy conclusions about significance and effect sizes, which is crucial for decision-making and scientific reproducibility. Multiple Imputation reinforces the idea that statistical analysis does not merely rely on available data, but also rigorously accounts for uncertainty, enabling more nuanced and transparent reporting of uncertainties in study conclusions.19\n\nMultiple Imputation is frequently described as a \"more rigorous approach\" 20 and a \"powerful tool\" 19 that \"better handles missing data\".17 This positions MI as a preferred or \"gold standard\" method. However, the requirement for correct model specification 19 and the computational intensity 19 introduce practical caveats. While MI offers theoretical superiority, its practical implementation demands careful model selection and can be computationally demanding, implying a trade-off between theoretical rigor and practical feasibility, especially for complex datasets or less experienced users. This suggests a need to balance theoretical optimality with practical constraints in real-world applications.\n\n## **4. Bayesian Multiple Imputation: Theoretical Framework and Algorithms**\n\n### **4.1 The Bayesian Perspective: Incorporating Prior Knowledge and Posterior Predictive Distributions**\n\nBayesian multiple imputation methods are fundamentally rooted in Bayesian statistics, a framework that updates the probability of a hypothesis based on new data using Bayes' theorem.1 This approach provides a natural and robust framework for modeling the uncertainty associated with missing data.1\n\nIn this context, missing values are filled in with draws from their posterior predictive distributions.21 This involves combining prior distributions for model parameters with the observed data likelihood.1 The joint posterior distribution in Bayesian imputation is expressed as:\n\nf(θ,ϕ,Ymis​∣Yobs​,R)∝f(θ)f(ϕ∣θ)f(Ycom​∣θ)f(R∣Ycom​,ϕ).21\n\nUnder the ignorable missingness assumption (Missing at Random, MAR, or Missing Completely at Random, MCAR), the posterior inference for the parameters of interest (θ) does not depend on the missingness model (ϕ).13 Consequently, Bayesian model-based imputation draws from the posterior predictive distribution\n\nf(Ymis​∣Yobs​) under the imputation model f(Ycom​∣θ)f(θ).13\n\nThe **role of prior distributions** is crucial as they allow for the incorporation of existing knowledge or beliefs about the missing data into the imputation model.1 This can enhance imputation accuracy, especially in smaller datasets or when strong domain expertise is available.1 The choice of prior (e.g., non-informative vs. informative) can significantly influence results.1 The\n\n**posterior predictive distribution** is the core of Bayesian imputation, representing the distribution of the unobserved (missing) data given the observed data. Imputations are drawn from this distribution, reflecting the uncertainty in both the missing values and the model parameters.13\n\nA critical aspect of Bayesian MI is its reliance on the \"ignorable missingness\" assumption. As stated in the literature, \"Under the ignorable missingness assumption... the posterior inference for θ does not depend on the missingness model\".21 This is a crucial simplification that underpins the practical tractability of many Bayesian MI applications. While Bayesian methods offer theoretical rigor, their practical implementation often relies on this strong assumption (MAR plus distinct parameters for data and missingness mechanism). If missingness is truly MNAR and not ignorable, the complexity of a fully Bayesian model becomes prohibitive 21, implying a practical boundary to \"fully Bayesian\" imputation and a need for sensitivity analysis for MNAR. The Bayesian approach to MI offers an epistemic advantage by formally integrating prior knowledge. This is not just about \"better\" estimates, but about making the imputation process more transparent and robust by explicitly stating and incorporating assumptions (priors). In situations with limited data or strong domain expertise, informative priors can stabilize imputation models and provide more realistic imputed values than purely data-driven (frequentist) methods. This moves beyond statistical mechanics to a more holistic knowledge integration.\n\n### **4.2 Key Algorithms and Computational Approaches**\n\nSeveral algorithms and computational approaches are employed in Bayesian Multiple Imputation:\n\n#### **4.2.1 Markov Chain Monte Carlo (MCMC) Methods**\n\nMCMC methods generate imputations by drawing from the posterior distribution of the missing data.19 A popular variant is the Data Augmentation algorithm.19\n\n* **Advantages:** MCMC directly models the uncertainty of the missing data mechanism, providing a theoretically sound approach.19 Gibbs sampling is a common MCMC algorithm used for generating multiple imputations from joint multivariate linear mixed models.13\n* **Limitations:** MCMC methods are computationally intensive, especially for large datasets or complex models.13 Convergence can be a concern, requiring careful monitoring to ensure stable and valid imputations.22\n\n#### **4.2.2 Fully Conditional Specification (FCS) or Chained Equations (MICE)**\n\nFCS, also known as Chained Equations or MICE (Multiple Imputation by Chained Equations), involves specifying an imputation model for each variable with missing values based on other variables in the dataset.13 This method is particularly useful when the data pattern is arbitrary and does not follow a monotone pattern.19\n\n* **Process:** FCS iteratively imputes missing values by cycling through each incomplete variable, modeling it conditionally on all other variables.13 This cycle is repeated for a pre-set number of iterations until convergence, and then the entire process is repeated 'm' times to generate multiple datasets.13\n* **Advantages:** FCS offers high flexibility in specifying different imputation models for different types of data (e.g., continuous, binary, categorical, multilevel).19 It performs well in practice, especially when conditional models align with the substantive model.13\n* **Limitations:** FCS can lead to inconsistent estimates if model specifications are not correctly aligned or if the conditional models do not correspond to a valid joint distribution.13 Its theoretical properties are less well understood compared to joint modeling approaches.13\n\nBoth MCMC and FCS/MICE are iterative algorithms. The validity of these imputation methods hinges on the algorithms reaching a stable state, or \"convergence.\" Non-convergence or slow convergence, especially in complex or high-dimensional datasets, is a significant practical challenge that can lead to invalid imputations and biased results, regardless of the theoretical soundness of the method. This underscores the need for diagnostic checks beyond simply running the code.\n\n### **4.3 Advantages and Computational Considerations of Bayesian Approaches**\n\nThe Bayesian approach to multiple imputation offers distinct advantages:\n\n* **Incorporation of Prior Information:** As discussed, the integration of prior information can enhance imputation accuracy, particularly in small samples or when strong domain knowledge exists.1\n* **Model and Parameter Uncertainty:** Bayesian methods, especially when combined with techniques like Bayesian Model Averaging (BMA), can account for both model and parameter uncertainty, leading to more robust inferences.22\n\nHowever, these advantages often come with **computational demands**. Bayesian methods, particularly MCMC, can be computationally intensive.19 Implementing fully Bayesian models can be complex.21 Techniques like Laplace approximation and Occam's Window algorithm are sometimes used to manage the computational burden in BMA.22 The computational demands of fully Bayesian imputation present a practical barrier. While theoretically superior in handling uncertainty and incorporating prior knowledge, the complexity can deter adoption. This creates a tension between statistical rigor and computational feasibility. The development of approximations 21 and efficient algorithms like MICE 22 reflects an ongoing effort to make Bayesian principles accessible and efficient for applied researchers. Often, \"not fully Bayesian\" approximations 21 or efficient algorithms like MICE are chosen as practical compromises that still offer significant advantages over simpler methods.\n\n## **5. Complete Case Analysis: Definition, Advantages, and General Limitations**\n\n### **5.1 Definition: Listwise Deletion and its Practical Implementation**\n\nComplete Case Analysis (CCA), also known as case or listwise deletion (LD), is one of the oldest and simplest methods for handling missing data.4 It involves discarding any unit or case that has incomplete information on\n\n*any* variable included in the analysis.4 Consequently, only cases with observed values for all variables under consideration are used in the analysis.5 CCA is notably the default method in many statistical software packages, including SPSS, SAS, Stata, and R's regression functions.6\n\n### **5.2 Primary Advantages: Simplicity and Ease of Use**\n\nThe primary appeal of CCA lies in its operational simplicity:\n\n* **Ease of Implementation:** CCA is remarkably easy to implement, often requiring just a single command or being the default behavior in statistical software.4 This convenience is a major reason for its widespread, albeit often uncritical, popularity.5\n* **Straightforward Comparison:** It allows for a direct comparison of different univariate statistics because they are calculated on a common set of cases.5\n* **Conceptual Simplicity:** The method is intuitively easy to understand and explain, requiring no complex imputation model specification.\n\n### **5.3 General Limitations: Data Loss, Bias, and Representativeness**\n\nDespite its simplicity, CCA is associated with significant limitations that can severely compromise the validity of statistical inferences:\n\n#### **5.3.1 Loss of Statistical Power and Efficiency**\n\nCCA can result in a significant loss of data, especially if the dataset has many missing values.2 For instance, a study noted a substantial 30% loss of patients in a prognostic model when using a complete case approach due to incomplete covariates.14 This reduction in sample size directly compromises the statistical power of the study, increasing the likelihood of Type II errors (failing to detect a true effect).2 It also leads to reduced efficiency of estimates, meaning larger standard errors for regression coefficients.3\n\n#### **5.3.2 Potential for Bias Under Various Missing Data Mechanisms**\n\nCCA produces unbiased estimates of means, variances, and regression weights *only* when the data are Missing Completely at Random (MCAR).3 Under MCAR, the complete cases are a truly random subsample of the original data.5 If the missing data mechanism is not MCAR (i.e., MAR or MNAR), CCA can lead to biased parameter estimates.3 This bias arises because the completers are no longer a random sample of all cases.5 The bias of CCA for population means depends on the proportion of completers and the extent to which complete and incomplete cases differ.5\n\n#### **5.3.3 Concerns Regarding Representativeness**\n\nWhen data are not MCAR, the subset of complete cases may not be representative of the original full sample.2 This can skew results and lead to erroneous conclusions.2 For example, if a sample predominantly consists of individuals from a specific demographic due to missingness, the findings may not accurately reflect the behaviors of the entire population.2 Comparing distributions of variables between complete and incomplete cases can help identify potential MCAR violations.5 Significant differences indicate that MCAR is invalid, and CCA estimates are potentially biased.5\n\nThe \"ease of implementation\" 5 of CCA, coupled with its default status in software 6, creates a \"default trap\" for researchers. While it saves immediate effort, it often incurs significant hidden costs in terms of compromised statistical validity (bias) and reduced inferential power (efficiency loss).3 This suggests that the default behavior of software can inadvertently promote suboptimal practices, highlighting the need for deeper methodological understanding by users. Researchers should actively override default CCA settings in software unless specific, well-justified conditions for its unbiasedness are met. The simplicity of CCA is rarely a sufficient justification for its use in complex real-world data.\n\nWhile CCA is often perceived to lead to reduced statistical efficiency and increased potential for bias 12, some research indicates that under specific MCAR and MAR conditions, CCA method estimates can remain unbiased and achieve precision similar to or even better than MI methods, with high statistical coverage.12 This challenges the generalized understanding of CCA's inefficiency and bias. While true in many cases, this suggests that the choice of method is not a simple hierarchy but a context-dependent decision, where \"better\" is defined by the specific missingness mechanism and the parameters of interest.\n\n## **6. Unbiasedness of Complete Case Analysis for Regression Coefficients**\n\n### **6.1 General Conditions for Bias**\n\nAs a general rule, CCA yields biased estimates for regression coefficients when the data are not MCAR.3 Specifically, bias is introduced if the probability of being a complete case (or having missingness) depends on the outcome variable, even after conditioning on covariates.5 This is a common scenario under MAR and MNAR mechanisms.7 The recurring theme is that the\n\n*dependency of missingness on the outcome* is the primary driver of bias in CCA for regression coefficients. If missingness is related to the outcome, then the complete cases are no longer representative of the full sample's outcome distribution, leading to skewed estimates of the relationship between predictors and the outcome. This is a crucial distinction from missingness only depending on observed covariates. Researchers must critically assess the missing data mechanism, particularly the relationship between missingness and the outcome, to determine the suitability of CCA.\n\n### **6.2 Specific Scenarios for Unbiasedness**\n\nDespite its general limitations, CCA can yield unbiased regression coefficients under specific, often counter-intuitive, conditions.8 This challenges the common misconception that MCAR is\n\n*always* required for unbiased CCA. The fact that CCA can be unbiased even under certain MNAR conditions is a critical nuance that allows for its legitimate use in highly specific, well-understood contexts.\n\n#### **6.2.1 Missing Data Confined Solely to the Outcome Variable (Y)**\n\nIf missing data occur *only* in the outcome variable (Y) and not in any predictors (X), then for estimating regression weights, CCA and Multiple Imputation are equivalent.16 In this specific situation, CCA is often preferred due to its simplicity, efficiency, and robustness for regression weights.16 However, it is important to note that quantities that depend on the correct marginal distribution of Y (e.g., the mean or R-squared) would still require the stronger MCAR assumption for unbiasedness.16\n\n#### **6.2.2 Probability of Missingness Independent of the Outcome Variable (Y)**\n\nIf the probability of data being missing does *not* depend on the outcome variable (Y), assuming the complete-data model is correctly specified, then the regression coefficients are free of bias.11 This holds true for\n\n*any type of regression analysis* and for missing data in *both Y and X*.16 Crucially, since the missing data rate\n\n*may* depend on covariates (X), CCA can actually work effectively in a relevant class of MNAR models.11 Simulations have confirmed this superiority in certain scenarios, though MI can gain advantage with more predictive variables.16 The underlying principle unifying these \"special cases\" where CCA is unbiased is the\n\n*conditional independence* between the outcome and the missingness indicator given the covariates in the model.8 This reframes the discussion from broad MDM categories to specific causal structures of missingness, demanding a deeper understanding of the data generation process.\n\n### **6.3 Implications for Categorical Outcome Variables: The Logistic Regression Special Case**\n\nFor logistic regression, which models categorical (typically binary) outcome variables, there is a distinct special case where CCA can yield unbiased estimates for regression coefficients (specifically, odds ratios).11\n\n**Conditions for Unbiasedness in Logistic Regression (Categorical Outcome):**\n\n* The missing data must be confined to *either* a dichotomous outcome (Y) *or* to the predictors (X), but *not to both*.16\n* Assuming the model is correctly specified, the regression coefficients (excluding the intercept) from the complete-case analysis are unbiased *if the probability of being missing depends only on Y and not on X*.16\n* This property provides the statistical basis for the unbiased estimation of odds ratios from case-control studies in epidemiology.16\n* **Crucial Limitation:** This property does *not* hold if missing data occur in both Y and X simultaneously.16\n\nThe unique unbiasedness property of CCA in logistic regression stems from the \"special symmetry property of the odds ratio measure\".24 Unlike linear regression coefficients, the odds ratio has a reciprocal relationship between outcome and exposure. This symmetry allows for unbiased estimation under specific missingness patterns that would bias other regression types. This is a deep theoretical property that translates into a practical advantage for CCA in specific epidemiological contexts. For studies employing logistic regression with categorical outcomes, CCA can be a viable and even preferred method if the missingness pattern aligns with these strict conditions (missing only in Y or X, and missingness dependent only on Y). This challenges the blanket recommendation against CCA.\n\nWhile MI usually relies on a MAR assumption to produce unbiased results, CCA can also produce unbiased results under more conditions than some analysts may realize, including certain MNAR conditions for logistic regression.15 In scenarios where both MI and CCA are valid (e.g., MNAR-predictable where missingness is not associated with the outcome), MI (specifically MI-FCS) was found to be at least as good as CCA in terms of bias and coverage, and superior when missingness involved a categorical covariate.15 Conversely, for MAR or MCAR outcomes, CCA estimates can remain unbiased and achieve precision similar to or better than MI methods in some assessed scenarios.12 MNAR scenarios often yield invalid inferences with both methods unless specific conditions are met.12\n\n### **Table 1: Conditions for Unbiased Regression Coefficients in Complete Case Analysis**\n\n| Condition/Scenario | Description of Missingness | Type of Regression | Parameter(s) Unbiased | Key Caveats/Assumptions | Supporting References |\n| --- | --- | --- | --- | --- | --- |\n| **Missing Y Only** | Missing data occurs *only* in the outcome variable (Y), not in any predictors (X). | Any Regression | Regression Coefficients (weights) | Quantities dependent on marginal Y (e.g., mean, R-squared) still require MCAR. | 16 |\n| **Missingness Independent of Y** | Probability of data being missing does *not* depend on the outcome variable (Y). Missingness *can* depend on predictors (X). | Any Regression | Regression Coefficients | Complete-data model must be correctly specified. Applies to a class of MNAR models. | 11 |\n| **Logistic Regression Special Case** | Missing data confined to *either* dichotomous Y *or* to X, but *not both*. Probability of missingness depends *only* on Y, not on X. | Logistic Regression | Regression Coefficients (Odds Ratios, excluding intercept) | Model must be correctly specified. Does NOT hold if missing data occur in both Y and X. | 11 |\n\n## **7. Comparative Analysis: Multiple Imputation vs. Complete Case Analysis**\n\n### **7.1 Assumptions Underlying Each Method**\n\nThe choice between MI and CCA is heavily influenced by the assumptions regarding the missing data mechanism.\n\n* **Complete Case Analysis (CCA):** Primarily relies on the Missing Completely at Random (MCAR) assumption for unbiased estimates of means, variances, and regression weights.3 However, as discussed in Section 6, it can be unbiased under specific MAR or MNAR conditions, particularly for regression coefficients when missingness is not outcome-dependent.11\n* **Multiple Imputation (MI):** Typically relies on a Missing at Random (MAR) assumption for valid inferences.12 It assumes that the missing data can be predicted from the observed data. Bayesian MI further incorporates prior distributions, allowing for the integration of existing knowledge into the imputation process.1 MI can also handle certain MNAR situations if the imputation model is correctly specified to account for the non-random missingness.1\n\nIt is generally impossible to distinguish between MAR and MNAR based solely on observed data; subject matter knowledge is crucial for making informed assumptions about the missingness mechanism.8 This inability to empirically distinguish between MAR and MNAR creates a fundamental practical dilemma for researchers. Since the choice of method (CCA vs. MI, and specific MI models) heavily depends on this unidentifiable mechanism, researchers are often forced to rely on strong, untestable assumptions or domain expertise. This highlights the inherent uncertainty in missing data handling, regardless of the method chosen. Consequently, sensitivity analyses (e.g., comparing MI and CCA results, or conducting MNAR sensitivity analysis) are not just good practice but a necessity to build confidence in conclusions when the missingness mechanism is uncertain.15\n\n### **7.2 Strengths and Weaknesses in Terms of Bias, Efficiency, and Robustness**\n\n| Feature/Criterion | Complete Case Analysis (CCA) | Multiple Imputation (MI) |\n| --- | --- | --- |\n| **Core Principle** | Discards any case with one or more missing values. | Replaces each missing value with multiple plausible estimates. |\n| **Primary Assumption for Unbiasedness** | Missing Completely at Random (MCAR) for general estimates. Specific conditions for regression coefficients (see Table 1). | Missing at Random (MAR) for general application. Can handle some MNAR with specialized models. |\n| **General Bias Risk** | High, unless MCAR or specific conditions apply. | Low, generally yields less biased estimates under MAR. |\n| **Statistical Power/Efficiency** | Significant data loss, reduced power, inflated standard errors. | Preserves sample size, higher power, more efficient estimates. |\n| **Handling of Imputation Uncertainty** | Ignores uncertainty, treating complete cases as the full dataset. | Explicitly accounts for uncertainty through multiple datasets and Rubin's Rules. |\n| **Ease of Implementation** | Very easy, often default in software. | More complex, requires specialized software/expertise. |\n| **Computational Demands** | Very low, computationally inexpensive. | Higher, can be intensive for large datasets or complex models. |\n| **Applicability to Different MDMs** | Limited to MCAR, or specific MAR/MNAR for regression coefficients. | Robust for MAR, can be adapted for some MNAR. |\n| **Suitability for Regression Coefficients (General)** | Biased unless specific conditions met (e.g., missing Y only, missingness independent of Y). | Generally preferred, less biased, especially under MAR. |\n| **Suitability for Marginal Estimates (Means, R-squared)** | Biased unless MCAR. | Generally preferred, provides more accurate estimates. |\n\nWhile MI is generally heralded as \"better\" 12, some research suggests that under specific MCAR/MAR conditions, CCA can perform similarly or even better in terms of precision and bias for certain estimates.12 This creates a paradox: the \"preferred\" method might not always offer a statistical advantage. This implies that the choice is not a simple hierarchy but a context-dependent decision, where \"better\" is defined by the specific missingness mechanism and the parameters of interest. A nuanced understanding of the missing data mechanism and the specific research question is crucial. Blindly applying MI as a universal panacea can be as suboptimal as blindly using CCA.\n\nThe primary advantage of CCA is its \"ease of implementation\".4 However, this simplicity often comes at the cost of statistical validity (bias) and efficiency (power loss).2 This highlights that simplicity in statistical methods often masks underlying complexities and potential pitfalls. The apparent \"cost-effectiveness\" of CCA (in terms of effort) can lead to a higher \"cost\" in terms of erroneous conclusions or missed effects, emphasizing a critical trade-off between methodological convenience and inferential accuracy.\n\n### **7.3 Guidance on Selecting the Appropriate Method**\n\nThe choice between MI and CCA should be a conscious decision, not a default.16\n\n* **When to Consider Multiple Imputation (MI):**\n  + When data are MAR or MNAR, and the missingness is outcome-dependent.7\n  + When preserving statistical power and efficiency is critical.14\n  + When the proportion of missing data is substantial (e.g., greater than 5%).9\n  + When auxiliary variables are available that can help predict missing values, as these can significantly improve imputation quality.3\n* **When to Consider Complete Case Analysis (CCA) (with Caution):**\n  + When data are truly MCAR.3\n  + When missing data are confined solely to the outcome variable (Y) for estimating regression weights.16\n  + When the probability of missingness does not depend on the outcome (Y), even if it depends on covariates (X).11\n  + For logistic regression with categorical outcomes, if missing data are confined to Y or X (but not both), and missingness depends only on Y.11\n  + When the proportion of missing data is very small.4\n\nWhile often presented as competing methods, the literature suggests that MI and CCA can serve complementary roles in robust missing data analysis. Running both, especially when CCA might be unbiased, can act as a form of sensitivity analysis, building confidence in results if they align, or signaling potential issues if they diverge.12\n\n## **8. Conclusion and Recommendations**\n\nMissing data represents a ubiquitous and significant challenge in statistical analysis, capable of introducing substantial bias and reducing statistical power if not handled appropriately. This report has explored two primary approaches: Multiple Imputation (MI) and Complete Case Analysis (CCA), emphasizing the theoretical depth of Bayesian MI and the nuanced conditions under which CCA can yield unbiased regression coefficients, particularly for categorical outcome variables.\n\nMultiple Imputation stands as a robust and generally preferred method, fundamentally shifting the approach to missing data from simple substitution to a comprehensive quantification of uncertainty. Its three-step process—imputation, analysis, and pooling—ensures that the variability introduced by missing values is properly accounted for, leading to more accurate estimates and valid inferences. The Bayesian perspective further enhances MI by allowing the formal incorporation of prior knowledge through posterior predictive distributions, offering a theoretically grounded framework for imputation, albeit with increased computational demands for fully Bayesian models. Algorithms like MCMC and FCS/MICE provide practical means to implement Bayesian MI, each with its own advantages in terms of theoretical purity versus practical flexibility.\n\nConversely, Complete Case Analysis, while simple and widely accessible as a default in many software packages, carries significant risks of data loss, reduced statistical power, and biased estimates. Its validity is strictly contingent on the missing data being Missing Completely at Random (MCAR), a condition rarely met in real-world datasets. However, a critical nuance often overlooked is that CCA *can* yield unbiased regression coefficients under specific, well-defined scenarios. These include situations where missingness is confined solely to the outcome variable, or where the probability of missingness does not depend on the outcome (even if it depends on predictors). For logistic regression with categorical outcomes, CCA can even be unbiased if missingness is restricted to either the outcome or predictors (but not both) and depends only on the outcome, leveraging the unique symmetry of the odds ratio. These specific conditions highlight that the unbiasedness of CCA is not a general rule but depends on the precise location and mechanism of missingness relative to the outcome and predictors, challenging the blanket assumption of its universal bias.\n\n### **Practical Recommendations for Researchers and Analysts:**\n\n1. **Assess Missing Data Mechanism Rigorously:** The initial step in any analysis involving missing data must be a thorough exploration of missingness patterns and a reasoned assessment of the underlying missing data mechanism (MCAR, MAR, or MNAR).1 This assessment, often relying on domain knowledge due to the unidentifiability of MNAR, is paramount for informed method selection.\n2. **Prioritize Multiple Imputation for MAR Data:** For data that are plausibly Missing at Random (MAR), Multiple Imputation is generally the preferred and most robust method. It offers superior bias reduction and more accurate uncertainty estimates compared to simpler alternatives.1\n3. **Use Complete Case Analysis Judiciously and with Justification:** CCA should only be employed when the proportion of missing data is very small AND the MCAR assumption is highly plausible, OR when the specific conditions for unbiased regression coefficients (as detailed in Table 1) are met and explicitly justified.4 Analysts must always acknowledge the potential for bias and power loss inherent in this method.\n4. **Consider Complementary Analyses and Sensitivity Checks:** For critical findings, it is advisable to perform both MI and CCA (when CCA is plausibly unbiased) to assess the robustness of results.12 Alignment between the results can build confidence, while discrepancies should prompt further investigation into the missingness mechanism or model specification.\n5. **Address MNAR with Advanced Methods or Sensitivity Analyses:** For suspected MNAR data, standard MI (which assumes MAR) and CCA are likely to yield biased estimates.4 This necessitates the use of more advanced, MNAR-specific modeling techniques or, at minimum, rigorous sensitivity analyses to explore the impact of different MNAR assumptions on the study conclusions.1\n6. **Document Missing Data Handling Transparently:** Regardless of the chosen method, transparently report the missing data mechanism assumed, the specific handling method employed, and any underlying assumptions made. This enhances the reproducibility and credibility of the research.\n\n### **Future Directions:**\n\nFuture research and practical development should focus on enhancing the robustness and computational efficiency of Bayesian MI algorithms, particularly for complex data structures (e.g., multilevel, longitudinal) and for more effectively addressing MNAR mechanisms. Furthermore, increased integration of sophisticated missing data diagnostics and sensitivity analysis tools into standard statistical software will be crucial to facilitate the adoption of best practices among a broader range of researchers.\n\n#### 引用的著作\n\n1. Mastering Multiple Imputation in Bayesian Stats - Number Analytics, 访问时间为 七月 29, 2025， <https://www.numberanalytics.com/blog/ultimate-guide-multiple-imputation-bayesian-statistics>\n2. Statistical Analysis Limitations - FasterCapital, 访问时间为 七月 29, 2025， <https://fastercapital.com/term/statistical-analysis-limitations.html>\n3. The use of complete-case and multiple imputation-based analyses in molecular epidemiology studies that assess interaction effects - PubMed Central, 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC3217865/>\n4. Listwise Deletion in Missing Data - GeeksforGeeks, 访问时间为 七月 29, 2025， <https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data/>\n5. Complete Case Analysis | Andrea Gabrio, 访问时间为 七月 29, 2025， <https://agabrioblog.onrender.com/missmethods/complete-case-analysis/>\n6. 1.3.1 Listwise deletion - Stef van Buuren, 访问时间为 七月 29, 2025， <https://stefvanbuuren.name/fimd/sec-simplesolutions.html>\n7. Multiple Imputation in Stata - OARC Stats - UCLA, 访问时间为 七月 29, 2025， <https://stats.oarc.ucla.edu/stata/seminars/mi_in_stata_pt1_new/>\n8. When Is a Complete-Case Approach to Missing Data Valid? The Importance of Effect-Measure Modification - PMC, 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC7705610/>\n9. Handling Missing Values Using Complete Case Analysis (CCA) | by Noor Fatima | Medium, 访问时间为 七月 29, 2025， [https://medium.com/@noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32](https://medium.com/%40noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32)\n10. www.geeksforgeeks.org, 访问时间为 七月 29, 2025， <https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data/#:~:text=Listwise%20Deletion%20works%20best%20under,bias%20and%20distort%20the%20analysis.>\n11. Accounting for missing data in statistical analyses: multiple imputation is not always the answer - PMC, 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC6693809/>\n12. (PDF) Is using multiple imputation better than complete case analysis for estimating a prevalence (risk) difference in randomized controlled trials when binary outcome observations are missing? - ResearchGate, 访问时间为 七月 29, 2025， <https://www.researchgate.net/publication/305625461_Is_using_multiple_imputation_better_than_complete_case_analysis_for_estimating_a_prevalence_risk_difference_in_randomized_controlled_trials_when_binary_outcome_observations_are_missing>\n13. Multiple imputation by chained equations for systematically and ..., 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC5496677/>\n14. Comparison of regression imputation methods of baseline ..., 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC8057424/>\n15. Approaches for missing covariate data in logistic regression with MNAR sensitivity analyses, 访问时间为 七月 29, 2025， <https://pubmed.ncbi.nlm.nih.gov/31957905/>\n16. 2.7 When not to use multiple imputation - Stef van Buuren, 访问时间为 七月 29, 2025， <https://stefvanbuuren.name/fimd/sec-when.html>\n17. Multiple Imputation: A Flexible Tool for Handling Missing Data - PMC, 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC4638176/>\n18. pmc.ncbi.nlm.nih.gov, 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC4957845/#:~:text=Complete%20case%20(CC)%20analysis%20is,as%20the%20preferred%20analytical%20approach.>\n19. Essential Guide to Multiple Imputation for Data Accuracy, 访问时间为 七月 29, 2025， <https://www.numberanalytics.com/blog/essential-guide-multiple-imputation>\n20. 3.2 Missing data options | Introduction to Regression Methods for ..., 访问时间为 七月 29, 2025， <https://www.bookdown.org/rwnahhas/RMPH/datasumm-missing.html>\n21. Diagnosing imputation models by applying target analyses to ..., 访问时间为 七月 29, 2025， <https://pmc.ncbi.nlm.nih.gov/articles/PMC4233994/>\n22. An Approach to Addressing Multiple Imputation Model Uncertainty ..., 访问时间为 七月 29, 2025， <https://bise.wceruw.org/documents/Kaplan_Yavuz.2019.pdf>\n23. Multiple Imputation in SAS Part 1 - OARC Stats - UCLA, 访问时间为 七月 29, 2025， <https://stats.oarc.ucla.edu/sas/seminars/multiple-imputation-in-sas/mi_new_1/>\n24. When is complete case/records logistic regression unbiased? - The Stats Geek, 访问时间为 七月 29, 2025， <https://thestatsgeek.com/2015/01/31/when-is-complete-caserecords-logistic-regression-unbiased/>\n",
  "all_source_links": [
    "https://accounts.google.com/SignOutOptions?hl=zh-CN&continue=https://gemini.google.com/%3Fis_sa%3D1%26is_sa%3D1%26android-min-version%3D301356232%26ios-min-version%3D322.0%26campaign_id%3Dbkws%26utm_source%3Dsem%26utm_source%3Dgoogle%26utm_medium%3Dpaid-media%26utm_medium%3Dcpc%26utm_campaign%3Dbkws%26utm_campaign%3D2024enAU_gemfeb%26pt%3D9008%26mt%3D8%26ct%3Dp-growth-sem-bkws%26gclsrc%3Daw.ds%26gad_source%3D1%26gad_campaignid%3D22437964261%26gbraid%3D0AAAAApk5BhmTIKeu82GeBI85qNScnwP3m%26gclid%3DCjwKCAjwkvbEBhApEiwAKUz6-ybVgRKghbvMe_m6J9HfhKccGbx5pv_fQCiPXewPYkQWthYXErb2txoClPAQAvD_BwE&ec=GBRAkgU",
    "https://agabrioblog.onrender.com/missmethods/complete-case-analysis",
    "https://bise.wceruw.org/documents/Kaplan_Yavuz.2019.pdf",
    "https://fastercapital.com/term/statistical-analysis-limitations.html",
    "https://medium.com/%40noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32",
    "https://medium.com/@noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32",
    "https://myactivity.google.com/product/gemini?utm_source=gemini",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC3217865",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4233994",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4638176",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4957845/#:~:text=Complete%20case%20(CC)%20analysis%20is,as%20the%20preferred%20analytical%20approach.",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC5496677",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC6693809",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC7705610",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC8057424",
    "https://policies.google.com/terms",
    "https://pubmed.ncbi.nlm.nih.gov/31957905",
    "https://stats.oarc.ucla.edu/sas/seminars/multiple-imputation-in-sas/mi_new_1",
    "https://stats.oarc.ucla.edu/stata/seminars/mi_in_stata_pt1_new",
    "https://stefvanbuuren.name/fimd/sec-simplesolutions.html",
    "https://stefvanbuuren.name/fimd/sec-when.html",
    "https://support.google.com/bard/answer/13594961",
    "https://support.google.com/gemini/answer/13594961#location&zippy=%2Cwhat-location-information-do-gemini-apps-collect-why-and-how-is-it-used",
    "https://thedecisionlab.com/biases/representativeness-heuristic",
    "https://thestatsgeek.com/2015/01/31/when-is-complete-caserecords-logistic-regression-unbiased",
    "https://www.bookdown.org/rwnahhas/RMPH/datasumm-missing.html",
    "https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data",
    "https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data/#:~:text=Listwise%20Deletion%20works%20best%20under,bias%20and%20distort%20the%20analysis.",
    "https://www.numberanalytics.com/blog/essential-guide-multiple-imputation",
    "https://www.numberanalytics.com/blog/ultimate-guide-multiple-imputation-bayesian-statistics",
    "https://www.researchgate.net/publication/305625461_Is_using_multiple_imputation_better_than_complete_case_analysis_for_estimating_a_prevalence_risk_difference_in_randomized_controlled_trials_when_binary_outcome_observations_are_missing",
    "https://www.researchgate.net/publication/361158443_Uncertainty_limits_the_use_of_power_analysis",
    "https://www.researchgate.net/publication/364535983_The_introduction_and_analysis_of_Representativeness_bias"
  ],
  "summary_citations": [
    "https://accounts.google.com/SignOutOptions?hl=zh-CN&continue=https://gemini.google.com/%3Fis_sa%3D1%26is_sa%3D1%26android-min-version%3D301356232%26ios-min-version%3D322.0%26campaign_id%3Dbkws%26utm_source%3Dsem%26utm_source%3Dgoogle%26utm_medium%3Dpaid-media%26utm_medium%3Dcpc%26utm_campaign%3Dbkws%26utm_campaign%3D2024enAU_gemfeb%26pt%3D9008%26mt%3D8%26ct%3Dp-growth-sem-bkws%26gclsrc%3Daw.ds%26gad_source%3D1%26gad_campaignid%3D22437964261%26gbraid%3D0AAAAApk5BhmTIKeu82GeBI85qNScnwP3m%26gclid%3DCjwKCAjwkvbEBhApEiwAKUz6-ybVgRKghbvMe_m6J9HfhKccGbx5pv_fQCiPXewPYkQWthYXErb2txoClPAQAvD_BwE&ec=GBRAkgU",
    "https://agabrioblog.onrender.com/missmethods/complete-case-analysis",
    "https://bise.wceruw.org/documents/Kaplan_Yavuz.2019.pdf",
    "https://fastercapital.com/term/statistical-analysis-limitations.html",
    "https://medium.com/%40noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32",
    "https://medium.com/@noorfatimaafzalbutt/handling-missing-values-using-complete-case-analysis-cca-2d73a7fc0c32",
    "https://myactivity.google.com/product/gemini?utm_source=gemini",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC3217865",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4233994",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4638176",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC4957845/#:~:text=Complete%20case%20(CC)%20analysis%20is,as%20the%20preferred%20analytical%20approach.",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC5496677",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC6693809",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC7705610",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC8057424",
    "https://policies.google.com/terms",
    "https://pubmed.ncbi.nlm.nih.gov/31957905",
    "https://stats.oarc.ucla.edu/sas/seminars/multiple-imputation-in-sas/mi_new_1",
    "https://stats.oarc.ucla.edu/stata/seminars/mi_in_stata_pt1_new",
    "https://stefvanbuuren.name/fimd/sec-simplesolutions.html",
    "https://stefvanbuuren.name/fimd/sec-when.html",
    "https://support.google.com/bard/answer/13594961",
    "https://support.google.com/gemini/answer/13594961#location&zippy=%2Cwhat-location-information-do-gemini-apps-collect-why-and-how-is-it-used",
    "https://thedecisionlab.com/biases/representativeness-heuristic",
    "https://thestatsgeek.com/2015/01/31/when-is-complete-caserecords-logistic-regression-unbiased",
    "https://www.bookdown.org/rwnahhas/RMPH/datasumm-missing.html",
    "https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data",
    "https://www.geeksforgeeks.org/data-science/listwise-deletion-in-missing-data/#:~:text=Listwise%20Deletion%20works%20best%20under,bias%20and%20distort%20the%20analysis.",
    "https://www.numberanalytics.com/blog/essential-guide-multiple-imputation",
    "https://www.numberanalytics.com/blog/ultimate-guide-multiple-imputation-bayesian-statistics",
    "https://www.researchgate.net/publication/305625461_Is_using_multiple_imputation_better_than_complete_case_analysis_for_estimating_a_prevalence_risk_difference_in_randomized_controlled_trials_when_binary_outcome_observations_are_missing",
    "https://www.researchgate.net/publication/361158443_Uncertainty_limits_the_use_of_power_analysis",
    "https://www.researchgate.net/publication/364535983_The_introduction_and_analysis_of_Representativeness_bias"
  ]
}